import java.util.Properties
 
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
 
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SQLContext
import org.apache.spark.rdd.RDD
 
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.clustering.{LDA, LDAModel}
import org.apache.spark.ml.feature.{HashingTF, Tokenizer, CountVectorizer, CountVectorizerModel, NGram, StopWordsRemover}
import org.apache.spark.ml.linalg.Vector
 
 
object scala_spark_nlp {
 
  // The raw transcripts are in a tuple format, capitalized words followed by accuracy scores and speaker indicators
  // These functions will convert the transcripts to more readable strings
  def convert : (String => String) = { s => s.replaceAll("[^A-Z,]", "")}
  val splt_fltr_trns_mk = (s:String) => { var s1 = s.split(","); var s2 = s1.filter(_!=""); var s3 = s2.mkString(" "); s3}
 
    def main(args: Array[String]) {
 
    // Setting up Spark Context
    val conf = new SparkConf().setAppName("Simple Application").set("spark.port.maxRetries", "100").set("spark.driver.memory", "10g").set("spark.executor.memory", "10g").set("spark.shuffle.service.enabled", "true").set("spark.dynamicAllocation.enabled", "true").set("spark.dynamicAllocation.maxExecutors", "500").set("spark.num.executors", "200")
    val sc = new SparkContext(conf)
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    // registering our user-defined functions from above
    import sqlContext.implicits._
    val converted = udf(convert)
    val converted2 = udf(splt_fltr_trns_mk)
    // read in the data
    val dat = sqlContext.read.parquet("/path/to/some/file")
    // Format to readable transcript
    val DF = dat.withColumn("new_trans", dat("transcript_list_clean").cast(StringType))
    val DF_with_formatted = DF.withColumn("formatted_transcript", converted(DF("new_trans")))
    val DF_with_convo = DF_with_formatted.withColumn("final_trans", converted2(DF_with_formatted("formatted_transcript")))
    val DF_lower = DF_with_convo.select("final_trans").rdd.map {
        case Row(string_val: String) => (string_val.toLowerCase)
    }.toDF("lower")
 
    // Use our own list of stopwords to filter out of the transcripts
    val custom_stopwords =  Array("a","about","above","across","after",
                            "again","against","all","almost","alone","along","already","also",
                            "although","always","among","an","and","another","any","anybody",
                            "anyone","anything","anywhere","are","area","areas","around","as",
                            "ask","asked","asking","asks","at","away","b","back",
                            "backed","backing","backs","be","became","because","become","becomes",
                            "been","before","began","behind","being","beings","best","better",
                            "between","big","both","but","by","c","came","can",
                            "cannot","case","cases","certain","certainly","clear","clearly","come",
                            "could","d","did","differ","different","differently","do","does","done",
                            "down","down","downed","downing","downs","during","e","each",
                            "early","either","end","ended","ending","ends","enough","even",
                            "evenly","ever","every","everybody","everyone","everything","everywhere","f",
                            "face","faces","fact","facts","far","felt","few","find",
                            "finds","first","for","four","from","full","fully","further",
                            "furthered","furthering","furthers","g","gave","general","generally","get",
                            "gets","give","given","gives","go","going","good","goods",
                            "got","great","greater","greatest","group","grouped","grouping","groups",
                            "h","had","has","have","having","he","her","here",
                            "herself","high","high","high","higher","highest","him","himself",
                            "his","how","however","i","if","important","in","interest",
                            "interested","interesting","interests","into","is","it","its","itself",
                            "j","just","k","keep","keeps","kind","knew","know",
                            "known","knows","l","large","largely","last","later","latest",
                            "known","knows","l","large","largely","last","later","latest",
                            "least","less","let","lets","like","likely","long","longer",
                            "longest","m","made","make","making","man","many","may",
                            "me","member","members","men","might","more","most","mostly",
                            "mr","mrs","much","must","my","myself","n","necessary",
                            "need","needed","needing","needs","never","new","new","newer",
                            "newest","next","no","nobody","non","noone","not","nothing",
                            "now","nowhere","number","numbers","o","of","off","often",
                            "old","older","oldest","on","once","one","only","open",
                            "opened","opening","opens","or","order","ordered","ordering","orders",
                            "other","others","our","out","over","p","part","parted",
                            "parting","parts","per","perhaps","place","places","point","pointed",
                            "pointing","points","possible","present","presented","presenting","presents","problem",
                            "problems","put","puts","q","quite","r","rather","really",
                            "right","right","room","rooms","s","said","same","saw",
                            "say","says","second","seconds","see","seem","seemed","seeming",
                            "seems","sees","several","shall","she","should","show","showed",
                            "showing","shows","side","sides","since","small","smaller","smallest",
                            "so","some","somebody","someone","something","somewhere","state","states",
                            "still","still","such","sure","t","take","taken","than",
                            "that","the","their","them","then","there","therefore","these",
                            "they","thing","things","think","thinks","this","those","though",
                            "thought","thoughts","three","through","thus","to","today","together",
                            "too","took","toward","turn","turned","turning","turns","two",
                            "u","under","until","up","upon","us","use","used",
                            "uses","v","very","w","want","wanted","wanting","wants",
                            "was","way","ways","we","well","wells","went","were",
                            "what","when","where","whether","which","while","who","whole",
                            "whose","why","will","with","within","without","work","worked",
                            "working","works","would","x","y","year","years","yet",
                            "you","young","younger","youngest","your","yours","z","um","umm",
                            "ummm","ummmm","uh","uhh","uhhh","uhhhh","hm","hmm","hmmm","hmmmm","mm","mmm","mmmm",
                            "oh", "ohh", "ohhh", "ohhhh", "ah", "ahh", "ahhh", "ahhhh", "uhuh", "uhhuh",
                            "yea", "yeah", "ok", "okay", "ya", "yah")
 
    // Pipeline an LDA model.
    //  1. tokenize, i.e. break each transcript into words
    //  2. remove stopwords
    //  3. use n-grams (n=2 means we use two-word phrases as our "words")
    //  4. vectorize
    //  5. fit the LDA model
    val tokenizer = new Tokenizer()
      .setInputCol("lower")
      .setOutputCol("words")
    val stopWordsRemover = new StopWordsRemover()
      .setInputCol("words")
      .setOutputCol("filtered")
      .setStopWords(custom_stopwords)
    val ngram = new NGram()
      .setN(2)
      .setInputCol("filtered")
      .setOutputCol("ngrams")
    val vectorizer = new CountVectorizer()
      .setInputCol("ngrams")
      .setOutputCol("features")
      .setVocabSize(40000)
      .setMinDF(5)
    val numClust = 20
    val lda = new LDA().setK(numClust).setMaxIter(10)
    val pipeline = new Pipeline()
      .setStages(Array(tokenizer, stopWordsRemover, ngram, vectorizer, lda))
    val pipelineModel = pipeline.fit(DF_lower)
    val transformed = pipelineModel.transform(DF_lower)
 
    // Describe topics.
    val vocab = pipelineModel.stages(3).asInstanceOf[CountVectorizerModel].vocabulary
    val numTopWords = 5
    val topicIndices = pipelineModel.stages(4).asInstanceOf[LDAModel].describeTopics(numTopWords)
    val listIndices = topicIndices.select("termIndices").rdd.map(r => r(0).asInstanceOf[Seq[Int]]).collect()
    val listWeights = topicIndices.select("termWeights").rdd.map(r => r(0).asInstanceOf[Seq[Int]]).collect()
    var tt:Int = 0
    var uu:Int = 0
    for ( tt <- 0 until numClust ) {
     println("Topic: "+(tt+1))
      for( uu <- 0 until numTopWords ) {
       println(vocab(listIndices(tt)(uu))+" " +listWeights(tt)(uu))
      }
    }
 
 
 
 
 
}
}
